# Â§ßËØ≠Ë®ÄÊ®°ÂûãÈÖçÁΩÆ (v0.1.7)

## Ê¶ÇËø∞

TradingAgents-CN Ê°ÜÊû∂ÊîØÊåÅÂ§öÁßçÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊèê‰æõÂïÜÔºåÂåÖÊã¨ DeepSeek„ÄÅÈòøÈáåÁôæÁÇº„ÄÅGoogle AI„ÄÅOpenAI Âíå Anthropic„ÄÇÊú¨ÊñáÊ°£ËØ¶ÁªÜ‰ªãÁªç‰∫ÜÂ¶Ç‰ΩïÈÖçÁΩÆÂíå‰ºòÂåñ‰∏çÂêåÁöÑ LLM ‰ª•Ëé∑ÂæóÊúÄ‰Ω≥ÊÄßËÉΩÂíåÊàêÊú¨ÊïàÁõä„ÄÇ

## üéØ v0.1.7 LLMÊîØÊåÅÊõ¥Êñ∞

- ‚úÖ **DeepSeek V3**: Êñ∞Â¢ûÊàêÊú¨‰ºòÂåñÁöÑ‰∏≠ÊñáÊ®°Âûã
- ‚úÖ **Êô∫ËÉΩË∑ØÁî±**: Ê†πÊçÆ‰ªªÂä°Ëá™Âä®ÈÄâÊã©ÊúÄ‰ºòÊ®°Âûã
- ‚úÖ **ÊàêÊú¨ÊéßÂà∂**: ËØ¶ÁªÜÁöÑÊàêÊú¨ÁõëÊéßÂíåÈôêÂà∂
- ‚úÖ **Â∑•ÂÖ∑Ë∞ÉÁî®**: ÂÆåÊï¥ÁöÑFunction CallingÊîØÊåÅ

## ÊîØÊåÅÁöÑ LLM Êèê‰æõÂïÜ

### 1. üá®üá≥ DeepSeek (v0.1.7Êñ∞Â¢ûÔºåÊé®Ëçê)

#### ÊîØÊåÅÁöÑÊ®°Âûã
```python
deepseek_models = {
    "deepseek-chat": {
        "description": "DeepSeek V3 ÂØπËØùÊ®°Âûã",
        "context_length": 64000,
        "cost_per_1k_tokens": {"input": 0.0014, "output": 0.0028},
        "recommended_for": ["‰∏≠ÊñáÂàÜÊûê", "Â∑•ÂÖ∑Ë∞ÉÁî®", "ÊàêÊú¨ÊïèÊÑüÂú∫ÊôØ"],
        "features": ["Â∑•ÂÖ∑Ë∞ÉÁî®", "‰∏≠Êñá‰ºòÂåñ", "Êï∞Â≠¶ËÆ°ÁÆó"]
    },
    "deepseek-coder": {
        "description": "DeepSeek ‰ª£Á†ÅÁîüÊàêÊ®°Âûã",
        "context_length": 64000,
        "cost_per_1k_tokens": {"input": 0.0014, "output": 0.0028},
        "recommended_for": ["‰ª£Á†ÅÂàÜÊûê", "ÊäÄÊúØÊåáÊ†áËÆ°ÁÆó", "Êï∞ÊçÆÂ§ÑÁêÜ"],
        "features": ["‰ª£Á†ÅÁîüÊàê", "ÈÄªËæëÊé®ÁêÜ", "Êï∞ÊçÆÂàÜÊûê"]
    }
}
```

#### ÈÖçÁΩÆÁ§∫‰æã
```bash
# .env ÈÖçÁΩÆ
DEEPSEEK_API_KEY=sk-your_deepseek_api_key_here
DEEPSEEK_ENABLED=true
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_BASE_URL=https://api.deepseek.com
```

#### ÁâπËâ≤ÂäüËÉΩ
- **üîß Â∑•ÂÖ∑Ë∞ÉÁî®**: Âº∫Â§ßÁöÑFunction CallingËÉΩÂäõ
- **üí∞ ÊàêÊú¨‰ºòÂåñ**: ÊØîGPT-4‰æøÂÆú90%‰ª•‰∏ä
- **üá®üá≥ ‰∏≠Êñá‰ºòÂåñ**: ‰∏ì‰∏∫‰∏≠ÊñáÂú∫ÊôØËÆæËÆ°
- **üìä Êï∞ÊçÆÂàÜÊûê**: ‰ºòÁßÄÁöÑÊï∞Â≠¶ÂíåÈÄªËæëÊé®ÁêÜËÉΩÂäõ

### 2. üá®üá≥ ÈòøÈáåÁôæÁÇº (Êé®Ëçê)

#### ÊîØÊåÅÁöÑÊ®°Âûã
```python
qwen_models = {
    "qwen-plus": {
        "description": "ÈÄö‰πâÂçÉÈóÆPlusÊ®°Âûã",
        "context_length": 32000,
        "cost_per_1k_tokens": {"input": 0.004, "output": 0.012},
        "recommended_for": ["‰∏≠ÊñáÁêÜËß£", "Âø´ÈÄüÂìçÂ∫î", "Êó•Â∏∏ÂàÜÊûê"],
        "features": ["‰∏≠Êñá‰ºòÂåñ", "ÂìçÂ∫îÂø´ÈÄü", "ÁêÜËß£ÂáÜÁ°Æ"]
    },
    "qwen-max": {
        "description": "ÈÄö‰πâÂçÉÈóÆMaxÊ®°Âûã",
        "context_length": 8000,
        "cost_per_1k_tokens": {"input": 0.02, "output": 0.06},
        "recommended_for": ["Â§çÊùÇÊé®ÁêÜ", "Ê∑±Â∫¶ÂàÜÊûê", "È´òË¥®ÈáèËæìÂá∫"],
        "features": ["Êé®ÁêÜËÉΩÂäõÂº∫", "ËæìÂá∫Ë¥®ÈáèÈ´ò", "ÈÄªËæëÊ∏ÖÊô∞"]
    }
}
```

### 3. üåç Google AI (Êé®Ëçê)

#### ÊîØÊåÅÁöÑÊ®°Âûã
```python
gemini_models = {
    "gemini-1.5-pro": {
        "description": "Gemini 1.5 ProÊ®°Âûã",
        "context_length": 1000000,
        "cost_per_1k_tokens": {"input": 0.0035, "output": 0.0105},
        "recommended_for": ["Â§çÊùÇÊé®ÁêÜ", "ÈïøÊñáÊú¨Â§ÑÁêÜ", "Â§öÊ®°ÊÄÅÂàÜÊûê"],
        "features": ["Ë∂ÖÈïø‰∏ä‰∏ãÊñá", "Êé®ÁêÜËÉΩÂäõÂº∫", "Â§öÊ®°ÊÄÅÊîØÊåÅ"]
    },
    "gemini-1.5-flash": {
        "description": "Gemini 1.5 FlashÊ®°Âûã",
        "context_length": 1000000,
        "cost_per_1k_tokens": {"input": 0.00035, "output": 0.00105},
        "recommended_for": ["Âø´ÈÄü‰ªªÂä°", "ÊâπÈáèÂ§ÑÁêÜ", "ÊàêÊú¨ÊïèÊÑü"],
        "features": ["ÂìçÂ∫îÂø´ÈÄü", "ÊàêÊú¨‰Ωé", "ÊÄßËÉΩÂùáË°°"]
    }
}
```

### 4. OpenAI

#### ÊîØÊåÅÁöÑÊ®°Âûã
```python
openai_models = {
    "gpt-4o": {
        "description": "ÊúÄÊñ∞ÁöÑ GPT-4 ‰ºòÂåñÁâàÊú¨",
        "context_length": 128000,
        "cost_per_1k_tokens": {"input": 0.005, "output": 0.015},
        "recommended_for": ["Ê∑±Â∫¶ÂàÜÊûê", "Â§çÊùÇÊé®ÁêÜ", "È´òË¥®ÈáèËæìÂá∫"]
    },
    "gpt-4o-mini": {
        "description": "ËΩªÈáèÁ∫ß GPT-4 ÁâàÊú¨",
        "context_length": 128000,
        "cost_per_1k_tokens": {"input": 0.00015, "output": 0.0006},
        "recommended_for": ["Âø´ÈÄü‰ªªÂä°", "ÊàêÊú¨ÊïèÊÑüÂú∫ÊôØ", "Â§ßÈáèAPIË∞ÉÁî®"]
    },
    "gpt-4-turbo": {
        "description": "GPT-4 Turbo ÁâàÊú¨",
        "context_length": 128000,
        "cost_per_1k_tokens": {"input": 0.01, "output": 0.03},
        "recommended_for": ["Âπ≥Ë°°ÊÄßËÉΩÂíåÊàêÊú¨", "Ê†áÂáÜÂàÜÊûê‰ªªÂä°"]
    },
    "gpt-3.5-turbo": {
        "description": "ÁªèÊµéÂÆûÁî®ÁöÑÈÄâÊã©",
        "context_length": 16385,
        "cost_per_1k_tokens": {"input": 0.0005, "output": 0.0015},
        "recommended_for": ["ÁÆÄÂçï‰ªªÂä°", "È¢ÑÁÆóÊúâÈôê", "Âø´ÈÄüÂìçÂ∫î"]
    }
}
```

#### ÈÖçÁΩÆÁ§∫‰æã
```python
# OpenAI ÈÖçÁΩÆ
openai_config = {
    "llm_provider": "openai",
    "backend_url": "https://api.openai.com/v1",
    "deep_think_llm": "gpt-4o",           # Áî®‰∫éÂ§çÊùÇÂàÜÊûê
    "quick_think_llm": "gpt-4o-mini",     # Áî®‰∫éÁÆÄÂçï‰ªªÂä°
    "api_key": os.getenv("OPENAI_API_KEY"),
    
    # Ê®°ÂûãÂèÇÊï∞
    "model_params": {
        "temperature": 0.1,               # ‰ΩéÊ∏©Â∫¶‰øùËØÅ‰∏ÄËá¥ÊÄß
        "max_tokens": 2000,               # ÊúÄÂ§ßËæìÂá∫ÈïøÂ∫¶
        "top_p": 0.9,                     # Ê†∏ÈááÊ†∑ÂèÇÊï∞
        "frequency_penalty": 0.0,         # È¢ëÁéáÊÉ©ÁΩö
        "presence_penalty": 0.0,          # Â≠òÂú®ÊÉ©ÁΩö
    },
    
    # ÈÄüÁéáÈôêÂà∂
    "rate_limits": {
        "requests_per_minute": 3500,      # ÊØèÂàÜÈíüËØ∑Ê±ÇÊï∞
        "tokens_per_minute": 90000,       # ÊØèÂàÜÈíütokenÊï∞
    },
    
    # ÈáçËØïÈÖçÁΩÆ
    "retry_config": {
        "max_retries": 3,
        "backoff_factor": 2,
        "timeout": 60
    }
}
```

### 2. Anthropic Claude

#### ÊîØÊåÅÁöÑÊ®°Âûã
```python
anthropic_models = {
    "claude-3-opus-20240229": {
        "description": "ÊúÄÂº∫Â§ßÁöÑ Claude Ê®°Âûã",
        "context_length": 200000,
        "cost_per_1k_tokens": {"input": 0.015, "output": 0.075},
        "recommended_for": ["ÊúÄÂ§çÊùÇÁöÑÂàÜÊûê", "È´òË¥®ÈáèÊé®ÁêÜ", "ÂàõÊÑè‰ªªÂä°"]
    },
    "claude-3-sonnet-20240229": {
        "description": "Âπ≥Ë°°ÊÄßËÉΩÂíåÊàêÊú¨",
        "context_length": 200000,
        "cost_per_1k_tokens": {"input": 0.003, "output": 0.015},
        "recommended_for": ["Ê†áÂáÜÂàÜÊûê‰ªªÂä°", "Âπ≥Ë°°‰ΩøÁî®Âú∫ÊôØ"]
    },
    "claude-3-haiku-20240307": {
        "description": "Âø´ÈÄü‰∏îÁªèÊµéÁöÑÈÄâÊã©",
        "context_length": 200000,
        "cost_per_1k_tokens": {"input": 0.00025, "output": 0.00125},
        "recommended_for": ["Âø´ÈÄü‰ªªÂä°", "Â§ßÈáèË∞ÉÁî®", "ÊàêÊú¨‰ºòÂåñ"]
    }
}
```

#### ÈÖçÁΩÆÁ§∫‰æã
```python
# Anthropic ÈÖçÁΩÆ
anthropic_config = {
    "llm_provider": "anthropic",
    "backend_url": "https://api.anthropic.com",
    "deep_think_llm": "claude-3-opus-20240229",
    "quick_think_llm": "claude-3-haiku-20240307",
    "api_key": os.getenv("ANTHROPIC_API_KEY"),
    
    # Ê®°ÂûãÂèÇÊï∞
    "model_params": {
        "temperature": 0.1,
        "max_tokens": 2000,
        "top_p": 0.9,
        "top_k": 40,
    },
    
    # ÈÄüÁéáÈôêÂà∂
    "rate_limits": {
        "requests_per_minute": 1000,
        "tokens_per_minute": 40000,
    }
}
```

### 3. Google AI (Gemini)

#### ÊîØÊåÅÁöÑÊ®°Âûã
```python
google_models = {
    "gemini-pro": {
        "description": "Google ÁöÑ‰∏ªÂäõÊ®°Âûã",
        "context_length": 32768,
        "cost_per_1k_tokens": {"input": 0.0005, "output": 0.0015},
        "recommended_for": ["Â§öÊ®°ÊÄÅ‰ªªÂä°", "‰ª£Á†ÅÂàÜÊûê", "Êé®ÁêÜ‰ªªÂä°"]
    },
    "gemini-pro-vision": {
        "description": "ÊîØÊåÅÂõæÂÉèÁöÑ Gemini ÁâàÊú¨",
        "context_length": 16384,
        "cost_per_1k_tokens": {"input": 0.0005, "output": 0.0015},
        "recommended_for": ["ÂõæË°®ÂàÜÊûê", "Â§öÊ®°ÊÄÅËæìÂÖ•"]
    },
    "gemini-2.0-flash": {
        "description": "ÊúÄÊñ∞ÁöÑÂø´ÈÄüÁâàÊú¨",
        "context_length": 32768,
        "cost_per_1k_tokens": {"input": 0.0002, "output": 0.0008},
        "recommended_for": ["Âø´ÈÄüÂìçÂ∫î", "ÂÆûÊó∂ÂàÜÊûê"]
    }
}
```

#### ÈÖçÁΩÆÁ§∫‰æã
```python
# Google AI ÈÖçÁΩÆ
google_config = {
    "llm_provider": "google",
    "backend_url": "https://generativelanguage.googleapis.com/v1",
    "deep_think_llm": "gemini-pro",
    "quick_think_llm": "gemini-2.0-flash",
    "api_key": os.getenv("GOOGLE_API_KEY"),
    
    # Ê®°ÂûãÂèÇÊï∞
    "model_params": {
        "temperature": 0.1,
        "max_output_tokens": 2000,
        "top_p": 0.9,
        "top_k": 40,
    }
}
```

## LLM ÈÄâÊã©Á≠ñÁï•

### Âü∫‰∫é‰ªªÂä°Á±ªÂûãÁöÑÈÄâÊã©
```python
class LLMSelector:
    """LLM ÈÄâÊã©Âô® - Ê†πÊçÆ‰ªªÂä°ÈÄâÊã©ÊúÄÈÄÇÂêàÁöÑÊ®°Âûã"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.task_model_mapping = self._initialize_task_mapping()
        
    def select_model(self, task_type: str, complexity: str = "medium") -> str:
        """Ê†πÊçÆ‰ªªÂä°Á±ªÂûãÂíåÂ§çÊùÇÂ∫¶ÈÄâÊã©Ê®°Âûã"""
        
        task_config = self.task_model_mapping.get(task_type, {})
        
        if complexity == "high":
            return task_config.get("high_complexity", self.config["deep_think_llm"])
        elif complexity == "low":
            return task_config.get("low_complexity", self.config["quick_think_llm"])
        else:
            return task_config.get("medium_complexity", self.config["deep_think_llm"])
    
    def _initialize_task_mapping(self) -> Dict:
        """ÂàùÂßãÂåñ‰ªªÂä°-Ê®°ÂûãÊò†Â∞Ñ"""
        return {
            "fundamental_analysis": {
                "high_complexity": "gpt-4o",
                "medium_complexity": "gpt-4o-mini",
                "low_complexity": "gpt-3.5-turbo"
            },
            "technical_analysis": {
                "high_complexity": "claude-3-opus-20240229",
                "medium_complexity": "claude-3-sonnet-20240229",
                "low_complexity": "claude-3-haiku-20240307"
            },
            "news_analysis": {
                "high_complexity": "gpt-4o",
                "medium_complexity": "gpt-4o-mini",
                "low_complexity": "gemini-pro"
            },
            "social_sentiment": {
                "high_complexity": "claude-3-sonnet-20240229",
                "medium_complexity": "gpt-4o-mini",
                "low_complexity": "gemini-2.0-flash"
            },
            "risk_assessment": {
                "high_complexity": "gpt-4o",
                "medium_complexity": "claude-3-sonnet-20240229",
                "low_complexity": "gpt-4o-mini"
            },
            "trading_decision": {
                "high_complexity": "gpt-4o",
                "medium_complexity": "gpt-4o",
                "low_complexity": "claude-3-sonnet-20240229"
            }
        }
```

### ÊàêÊú¨‰ºòÂåñÁ≠ñÁï•
```python
class CostOptimizer:
    """ÊàêÊú¨‰ºòÂåñÂô® - Âú®ÊÄßËÉΩÂíåÊàêÊú¨Èó¥ÊâæÂà∞Âπ≥Ë°°"""
    
    def __init__(self, budget_config: Dict):
        self.daily_budget = budget_config.get("daily_budget", 100)  # ÁæéÂÖÉ
        self.cost_tracking = {}
        self.model_costs = self._load_model_costs()
        
    def get_cost_optimized_config(self, current_usage: Dict) -> Dict:
        """Ëé∑ÂèñÊàêÊú¨‰ºòÂåñÁöÑÈÖçÁΩÆ"""
        
        remaining_budget = self._calculate_remaining_budget(current_usage)
        
        if remaining_budget > 50:  # È¢ÑÁÆóÂÖÖË∂≥
            return {
                "deep_think_llm": "gpt-4o",
                "quick_think_llm": "gpt-4o-mini",
                "max_debate_rounds": 3
            }
        elif remaining_budget > 20:  # È¢ÑÁÆó‰∏≠Á≠â
            return {
                "deep_think_llm": "gpt-4o-mini",
                "quick_think_llm": "gpt-4o-mini",
                "max_debate_rounds": 2
            }
        else:  # È¢ÑÁÆóÁ¥ßÂº†
            return {
                "deep_think_llm": "gpt-3.5-turbo",
                "quick_think_llm": "gpt-3.5-turbo",
                "max_debate_rounds": 1
            }
    
    def estimate_request_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:
        """‰º∞ÁÆóËØ∑Ê±ÇÊàêÊú¨"""
        
        model_cost = self.model_costs.get(model, {"input": 0.001, "output": 0.002})
        
        input_cost = (input_tokens / 1000) * model_cost["input"]
        output_cost = (output_tokens / 1000) * model_cost["output"]
        
        return input_cost + output_cost
```

## ÊÄßËÉΩ‰ºòÂåñ

### ÊèêÁ§∫ËØç‰ºòÂåñ
```python
class PromptOptimizer:
    """ÊèêÁ§∫ËØç‰ºòÂåñÂô®"""
    
    def __init__(self):
        self.prompt_templates = self._load_prompt_templates()
        
    def optimize_prompt(self, task_type: str, model: str, context: Dict) -> str:
        """‰ºòÂåñÊèêÁ§∫ËØç"""
        
        base_prompt = self.prompt_templates[task_type]["base"]
        
        # Ê†πÊçÆÊ®°ÂûãÁâπÁÇπË∞ÉÊï¥ÊèêÁ§∫ËØç
        if "gpt" in model.lower():
            optimized_prompt = self._optimize_for_gpt(base_prompt, context)
        elif "claude" in model.lower():
            optimized_prompt = self._optimize_for_claude(base_prompt, context)
        elif "gemini" in model.lower():
            optimized_prompt = self._optimize_for_gemini(base_prompt, context)
        else:
            optimized_prompt = base_prompt
        
        return optimized_prompt
    
    def _optimize_for_gpt(self, prompt: str, context: Dict) -> str:
        """‰∏∫ GPT Ê®°Âûã‰ºòÂåñÊèêÁ§∫ËØç"""
        
        # GPT ÂñúÊ¨¢ÁªìÊûÑÂåñÁöÑÊåá‰ª§
        structured_prompt = f"""
‰ªªÂä°: {context.get('task_description', '')}

Êåá‰ª§:
1. ‰ªîÁªÜÂàÜÊûêÊèê‰æõÁöÑÊï∞ÊçÆ
2. Â∫îÁî®Áõ∏ÂÖ≥ÁöÑÈáëËûçÂàÜÊûêÊñπÊ≥ï
3. Êèê‰æõÊ∏ÖÊô∞ÁöÑÁªìËÆ∫ÂíåÂª∫ËÆÆ
4. ÂåÖÂê´ÁΩÆ‰ø°Â∫¶ËØÑ‰º∞

Êï∞ÊçÆ:
{context.get('data', '')}

ËØ∑ÊåâÁÖß‰ª•‰∏ãÊ†ºÂºèÂõûÁ≠î:
- ÂàÜÊûêÁªìÊûú: [‰Ω†ÁöÑÂàÜÊûê]
- ÁªìËÆ∫: [‰∏ªË¶ÅÁªìËÆ∫]
- Âª∫ËÆÆ: [ÂÖ∑‰ΩìÂª∫ËÆÆ]
- ÁΩÆ‰ø°Â∫¶: [0-1‰πãÈó¥ÁöÑÊï∞ÂÄº]
"""
        return structured_prompt
    
    def _optimize_for_claude(self, prompt: str, context: Dict) -> str:
        """‰∏∫ Claude Ê®°Âûã‰ºòÂåñÊèêÁ§∫ËØç"""
        
        # Claude ÂñúÊ¨¢ÂØπËØùÂºèÁöÑÊèêÁ§∫
        conversational_prompt = f"""
ÊàëÈúÄË¶Å‰Ω†‰Ωú‰∏∫‰∏Ä‰∏™‰∏ì‰∏öÁöÑÈáëËûçÂàÜÊûêÂ∏àÊù•Â∏ÆÂä©ÊàëÂàÜÊûê‰ª•‰∏ãÊï∞ÊçÆ„ÄÇ

{context.get('data', '')}

ËØ∑‰Ω†:
1. Ê∑±ÂÖ•ÂàÜÊûêËøô‰∫õÊï∞ÊçÆÁöÑÂê´‰πâ
2. ËØÜÂà´ÂÖ≥ÈîÆÁöÑË∂ãÂäøÂíåÊ®°Âºè
3. ËØÑ‰º∞ÊΩúÂú®ÁöÑÈ£éÈô©ÂíåÊú∫‰ºö
4. ÁªôÂá∫‰Ω†ÁöÑ‰∏ì‰∏öÂª∫ËÆÆ

ËØ∑Áî®‰∏ì‰∏ö‰ΩÜÊòìÊáÇÁöÑËØ≠Ë®ÄÂõûÁ≠îÔºåÂπ∂Ëß£Èáä‰Ω†ÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇ
"""
        return conversational_prompt
```

### Âπ∂ÂèëÊéßÂà∂
```python
class LLMConcurrencyManager:
    """LLM Âπ∂ÂèëÁÆ°ÁêÜÂô®"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.semaphores = self._initialize_semaphores()
        self.rate_limiters = self._initialize_rate_limiters()
        
    def _initialize_semaphores(self) -> Dict:
        """ÂàùÂßãÂåñ‰ø°Âè∑ÈáèÊéßÂà∂Âπ∂Âèë"""
        return {
            "openai": asyncio.Semaphore(10),      # OpenAI ÊúÄÂ§ö10‰∏™Âπ∂Âèë
            "anthropic": asyncio.Semaphore(5),    # Anthropic ÊúÄÂ§ö5‰∏™Âπ∂Âèë
            "google": asyncio.Semaphore(8)        # Google ÊúÄÂ§ö8‰∏™Âπ∂Âèë
        }
    
    async def execute_with_concurrency_control(self, provider: str, llm_call: callable) -> Any:
        """Âú®Âπ∂ÂèëÊéßÂà∂‰∏ãÊâßË°åLLMË∞ÉÁî®"""
        
        semaphore = self.semaphores.get(provider)
        rate_limiter = self.rate_limiters.get(provider)
        
        async with semaphore:
            await rate_limiter.acquire()
            try:
                result = await llm_call()
                return result
            except Exception as e:
                # Â§ÑÁêÜÈÄüÁéáÈôêÂà∂ÈîôËØØ
                if "rate_limit" in str(e).lower():
                    await asyncio.sleep(60)  # Á≠âÂæÖ1ÂàÜÈíü
                    return await llm_call()
                else:
                    raise e
```

## ÁõëÊéßÂíåË∞ÉËØï

### LLM ÊÄßËÉΩÁõëÊéß
```python
class LLMMonitor:
    """LLM ÊÄßËÉΩÁõëÊéß"""
    
    def __init__(self):
        self.metrics = {
            "request_count": defaultdict(int),
            "response_times": defaultdict(list),
            "token_usage": defaultdict(dict),
            "error_rates": defaultdict(float),
            "costs": defaultdict(float)
        }
    
    def record_request(self, model: str, response_time: float, 
                      input_tokens: int, output_tokens: int, cost: float):
        """ËÆ∞ÂΩïËØ∑Ê±ÇÊåáÊ†á"""
        
        self.metrics["request_count"][model] += 1
        self.metrics["response_times"][model].append(response_time)
        
        if model not in self.metrics["token_usage"]:
            self.metrics["token_usage"][model] = {"input": 0, "output": 0}
        
        self.metrics["token_usage"][model]["input"] += input_tokens
        self.metrics["token_usage"][model]["output"] += output_tokens
        self.metrics["costs"][model] += cost
    
    def get_performance_report(self) -> Dict:
        """Ëé∑ÂèñÊÄßËÉΩÊä•Âëä"""
        
        report = {}
        
        for model in self.metrics["request_count"]:
            response_times = self.metrics["response_times"][model]
            
            report[model] = {
                "total_requests": self.metrics["request_count"][model],
                "avg_response_time": sum(response_times) / len(response_times) if response_times else 0,
                "total_input_tokens": self.metrics["token_usage"][model].get("input", 0),
                "total_output_tokens": self.metrics["token_usage"][model].get("output", 0),
                "total_cost": self.metrics["costs"][model],
                "avg_cost_per_request": self.metrics["costs"][model] / self.metrics["request_count"][model] if self.metrics["request_count"][model] > 0 else 0
            }
        
        return report
```

## ÊúÄ‰Ω≥ÂÆûË∑µ

### 1. Ê®°ÂûãÈÄâÊã©Âª∫ËÆÆ
- **È´òÁ≤æÂ∫¶‰ªªÂä°**: ‰ΩøÁî® GPT-4o Êàñ Claude-3-Opus
- **Âπ≥Ë°°Âú∫ÊôØ**: ‰ΩøÁî® GPT-4o-mini Êàñ Claude-3-Sonnet  
- **ÊàêÊú¨ÊïèÊÑü**: ‰ΩøÁî® GPT-3.5-turbo Êàñ Claude-3-Haiku
- **Âø´ÈÄüÂìçÂ∫î**: ‰ΩøÁî® Gemini-2.0-flash

### 2. ÊàêÊú¨ÊéßÂà∂Á≠ñÁï•
- ËÆæÁΩÆÊØèÊó•È¢ÑÁÆóÈôêÂà∂
- ‰ΩøÁî®ËæÉÂ∞èÊ®°ÂûãÂ§ÑÁêÜÁÆÄÂçï‰ªªÂä°
- ÂÆûÊñΩÊô∫ËÉΩÁºìÂ≠òÂáèÂ∞ëÈáçÂ§çË∞ÉÁî®
- ÁõëÊéßtoken‰ΩøÁî®Èáè

### 3. ÊÄßËÉΩ‰ºòÂåñÊäÄÂ∑ß
- ‰ºòÂåñÊèêÁ§∫ËØçÈïøÂ∫¶ÂíåÁªìÊûÑ
- ‰ΩøÁî®ÈÄÇÂΩìÁöÑÊ∏©Â∫¶ÂèÇÊï∞
- ÂÆûÊñΩÂπ∂ÂèëÊéßÂà∂ÈÅøÂÖçÈÄüÁéáÈôêÂà∂
- ÂÆöÊúüÁõëÊéßÂíåË∞ÉÊï¥ÈÖçÁΩÆ

ÈÄöËøáÂêàÁêÜÁöÑLLMÈÖçÁΩÆÂíå‰ºòÂåñÔºåÂèØ‰ª•Âú®‰øùËØÅÂàÜÊûêË¥®ÈáèÁöÑÂêåÊó∂ÊéßÂà∂ÊàêÊú¨Âπ∂ÊèêÈ´òÁ≥ªÁªüÊÄßËÉΩ„ÄÇ
